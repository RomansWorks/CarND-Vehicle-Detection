{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# General Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "SETTING_RETRAIN_CLASSIFIER = False         # Change this to retrain the classifier\n",
    "SETTING_SAVE_RETRAINED_CLASSIFIER = True   # Change this to save the retrained classifier (if retraining)\n",
    "SETTING_LOAD_TRAINED_CLASSIFIER = True     # Change this in case SETTING_SAVE_RETRAINED_CLASSIFIER and you want to use the freshly trained classifier without overwriting the one on disc\n",
    "SETTING_TEST_CLASSIFIER_ON_TEST_IMAGES = False  # Change this to show how the classifier works on test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Note: Uses OpenCV 3.2 with Contrib (pip install opencv-contrib-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rbaron/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# SKLearn\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn import svm\n",
    "from skimage.feature import hog\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# MatPlotLib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "# The rest\n",
    "import numpy as np\n",
    "from scipy.ndimage.measurements import label\n",
    "import cv2\n",
    "from glob import glob\n",
    "import random\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Define Feature Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def convert_color(img, color_space='LUV'):\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: \n",
    "        feature_image = np.copy(img)\n",
    "    return feature_image   \n",
    "\n",
    "def get_hog_features(img, \n",
    "                     orient,\n",
    "                     pix_per_cell, \n",
    "                     cell_per_block,\n",
    "                     vis=False, \n",
    "                     feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=False, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=False, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "\n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    color1 = cv2.resize(img[:,:,0], size, interpolation=cv2.INTER_LINEAR).ravel()\n",
    "    color2 = cv2.resize(img[:,:,1], size, interpolation=cv2.INTER_LINEAR).ravel()\n",
    "    color3 = cv2.resize(img[:,:,2], size, interpolation=cv2.INTER_LINEAR).ravel()\n",
    "    return np.hstack((color1, color2, color3))\n",
    "                        \n",
    "def color_hist(img, nbins=32, bins_range=(0, 256)):    #\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    return hist_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (64, 64)\n",
    "\n",
    "color_space = 'LUV' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 12  # HOG orientations\n",
    "pix_per_cell = 8 # HOG pixels per cell\n",
    "cell_per_block = 1 # HOG cells per block\n",
    "hog_channel = 'ALL' # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (48, 48) # Spatial binning dimensions\n",
    "hist_bins = 64    # Number of histogram bins\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = True # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_features(img, color_space=color_space, spatial_size=spatial_size, hist_bins=hist_bins, orient=orient, \n",
    "                        pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, hog_channel=hog_channel,\n",
    "                        spatial_feat=spatial_feat, hist_feat=hist_feat, hog_feat=hog_feat):   \n",
    "    img_resize = cv2.resize(img, spatial_size, interpolation=cv2.INTER_LINEAR)\n",
    "    img_resize = (np.sqrt(img_resize.astype(np.float32)/255)*255).astype(np.uint8)    \n",
    "    file_features = []   \n",
    "    feature_image = convert_color(img_resize, color_space=color_space)\n",
    "    file_features.append(feature_image.ravel())\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        file_features.append(spatial_features)\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        file_features.append(hist_features)\n",
    "    if hog_feat == True:\n",
    "    # Call get_hog_features() with vis=False, feature_vec=True\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block))\n",
    "            hog_features = np.ravel(hog_features)        \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block)\n",
    "        # Append the new feature vector to the features list\n",
    "    file_features.append(hog_features)   \n",
    "    #  print(len(file_features), len(spatial_features), len(hist_features), len(hog_features)) \n",
    "    return np.concatenate(file_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load training data for the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if SETTING_RETRAIN_CLASSIFIER == True:\n",
    "    # total_imgs = 0\n",
    "    SAMPLES_TO_USE = 8000\n",
    "\n",
    "    vehicle_fnames = glob('dataset/vehicles/*/*.png')\n",
    "    non_vehicle_fnames = glob('dataset/non-vehicles/*/*.png')\n",
    "\n",
    "    vehicle_fnames = random.sample(vehicle_fnames, SAMPLES_TO_USE)\n",
    "    non_vehicle_fnames = random.sample(non_vehicle_fnames, SAMPLES_TO_USE)\n",
    "\n",
    "    print(\"Sampling complete, loading images\")\n",
    "\n",
    "    car_images = [mpimg.imread(fname) for fname in vehicle_fnames]\n",
    "    non_car_images = [mpimg.imread(fname) for fname in non_vehicle_fnames]\n",
    "\n",
    "    # Normalize\n",
    "    car_images = [(image.astype(np.float32)/np.max(image)*255).astype(np.uint8) for image in car_images]\n",
    "    non_car_images = [(image.astype(np.float32)/np.max(image)*255).astype(np.uint8) for image in non_car_images]\n",
    "\n",
    "    print(\"Loading complete, extracting features\")\n",
    "\n",
    "    car_features = [extract_features(image) for image in car_images]\n",
    "    non_car_features = [extract_features(image) for image in non_car_images]\n",
    "\n",
    "    print(\"Extracting complete\")\n",
    "\n",
    "    X_cars = np.vstack(car_features)\n",
    "    y_cars = np.ones(X_cars.shape[0], dtype=np.uint8)\n",
    "    X_non_cars = np.vstack(non_car_features)\n",
    "    y_non_cars = np.zeros(X_non_cars.shape[0], dtype=np.uint8)\n",
    "\n",
    "    X = np.vstack((X_cars, X_non_cars))\n",
    "    y = np.concatenate((y_cars, y_non_cars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Normalize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if SETTING_RETRAIN_CLASSIFIER == True:\n",
    "    # Fit a per-column scaler\n",
    "    X_scaler = StandardScaler().fit(X)\n",
    "\n",
    "    # Apply the scaler to X\n",
    "    scaled_X = X_scaler.transform(X)\n",
    "\n",
    "    print('{} cars labeled & {} non-cars labeled'.format(len(X_cars), len(X_non_cars)))\n",
    "    del X_cars\n",
    "    del X_non_cars\n",
    "\n",
    "    # Split up data into randomized training and test sets\n",
    "    rand_state = np.random.randint(0, 100)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=rand_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# After trying an (nearly) infite number of kernels via grid search... I  settle on the best performing one\n",
    "SEARCH_C_VALUES=[0.0001]\n",
    "SEARCH_KERNEL_VALUES=['linear'] \n",
    "SEARCH_GAMMA_VALUES=[0.00001]\n",
    "parameters = {'kernel': SEARCH_KERNEL_VALUES, 'C': SEARCH_C_VALUES, 'gamma': SEARCH_GAMMA_VALUES}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Grid search (commented out because we have a good parameter combination now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# def optimize_param(parameters):\n",
    "#     svc = GridSearchCV(svm.SVC(), parameters)\n",
    "#     # Check the training time for the SVC\n",
    "#     t=time.time()\n",
    "#     svc.fit(X_train, y_train)\n",
    "#     t2 = time.time()\n",
    "#     print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "#     optimal_params = svc.best_params_\n",
    "#     print(\"Optimal parameters found: \", optimal_params, \"\\n\")\n",
    "#     # Check the score of the SVC\n",
    "#     test_score = round(svc.score(X_test, y_test), 4)\n",
    "#     print('Test Accuracy of SVC = ', test_score)\n",
    "#     # Check the prediction time for a single sample\n",
    "#     t=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Final training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if SETTING_RETRAIN_CLASSIFIER == True:\n",
    "    print('Using:',orient,'orientations',pix_per_cell,\n",
    "        'pixels per cell and', cell_per_block,'cells per block')\n",
    "    print('Feature vector length:', len(X_train[0]))\n",
    "\n",
    "    svca = LinearSVC(C=0.0001, dual=False, max_iter=5)\n",
    "\n",
    "\n",
    "    Batch_size = len(X_train)//5\n",
    "    print('training on {} samples, {} batches, each batch {} samples'.format(len(X_train), len(X_train)//Batch_size, Batch_size))\n",
    "\n",
    "    for batch in range(0,len(X_train)//Batch_size):\n",
    "    # for batch in range(0,2):\n",
    "        t=time.time()\n",
    "        svca.fit(X_train[batch*Batch_size:(batch+1)*Batch_size], y_train[batch*Batch_size:(batch+1)*Batch_size])\n",
    "        t2 = time.time()\n",
    "        print('batch:', batch+1,'-', round(t2-t, 2), 'Seconds to train SVC...')\n",
    "\n",
    "        # Check the score of the SVC\n",
    "        test_score = round(svca.score(X_test, y_test), 4)\n",
    "        print('Test Accuracy of SVC = ', test_score)\n",
    "        # Check the prediction time for a single sample\n",
    "        t=time.time()\n",
    "    print ('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Print statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if SETTING_RETRAIN_CLASSIFIER == True:\n",
    "    predict = svca.predict(X_test)\n",
    "\n",
    "    def print_stats(labels, predict):\n",
    "        print(labels[:20])\n",
    "        print(predict[:20])\n",
    "        cm = confusion_matrix(labels, predict)\n",
    "        tot = cm.sum()\n",
    "        TN = cm[0][0]/tot\n",
    "        FP = cm[0][1]/tot\n",
    "        FN = cm[1][0]/tot\n",
    "        TP = cm[1][1]/tot\n",
    "        print(\"%s %.2f%% %s %.2f%% %s %.2f%% %s %.2f%%\\n\" % ('TP:',TP, 'FP:',FP, 'TN:',TN, 'FN:',FN))   \n",
    "        print(classification_report(labels, predict))\n",
    "\n",
    "    print_stats(y_test, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Save the classifier if we're happy with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "CLASSIFIER_SAVE_PATH = 'intermediates/car_classifier.pkl'\n",
    "SCALER_SAVE_PATH = 'intermediates/car_scaler.pkl'\n",
    "\n",
    "if SETTING_SAVE_RETRAINED_CLASSIFIER == True and SETTING_RETRAIN_CLASSIFIER == True:\n",
    "    joblib.dump(svca, CLASSIFIER_SAVE_PATH)\n",
    "    joblib.dump(X_scaler, SCALER_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load the saved classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier loaded successfully:\n",
      "LinearSVC(C=0.0001, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=5,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "Scaler loaded successfully:\n",
      "StandardScaler(copy=True, with_mean=True, with_std=True)\n"
     ]
    }
   ],
   "source": [
    "SETTING_LOAD_TRAINED_CLASSIFIER = True\n",
    "CLASSIFIER_LOAD_PATH = 'intermediates/car_classifier.pkl'\n",
    "SCALER_LOAD_PATH = 'intermediates/car_scaler.pkl'\n",
    "if SETTING_LOAD_TRAINED_CLASSIFIER == True:\n",
    "    svc = None\n",
    "    svc = joblib.load(CLASSIFIER_LOAD_PATH)\n",
    "    if svc != None:\n",
    "        print('Classifier loaded successfully:')\n",
    "        print(svc)\n",
    "    X_scaler = joblib.load(SCALER_LOAD_PATH)\n",
    "    if X_scaler != None:\n",
    "        print('Scaler loaded successfully:')\n",
    "        print(X_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Define the window search and conversion to bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# WINDOW_SEARCH_RANGES = [\n",
    "# #      {'window_size': (256,256), 'x_start_stop': [None, None], 'y_start_stop': [360, 740], 'xy_overlap':(0.8, 0.8)},\n",
    "#      {'window_size': (192,192), 'x_start_stop': [None, None], 'y_start_stop': [360, 740], 'xy_overlap':(0.8, 0.8)},\n",
    "#      {'window_size': (128,128), 'x_start_stop': [50, 1300], 'y_start_stop': [360, 600], 'xy_overlap':(0.8, 0.8)},\n",
    "#      {'window_size': (64,64), 'x_start_stop': [200, 1200], 'y_start_stop': [360, 500], 'xy_overlap':(0.85, 0.85)},\n",
    "# ]\n",
    "\n",
    "WINDOW_SEARCH_RANGES = [\n",
    "#      {'window_size': (256,256), 'x_start_stop': [None, None], 'y_start_stop': [360, 740], 'xy_overlap':(0.8, 0.8)},\n",
    "     {'window_size': (192,192), 'x_start_stop': [None, None], 'y_start_stop': [360, 740], 'xy_overlap':(0.7, 0.7)},\n",
    "     {'window_size': (160,160), 'x_start_stop': [None, None], 'y_start_stop': [360, 740], 'xy_overlap':(0.7, 0.7)},\n",
    "     {'window_size': (128,128), 'x_start_stop': [None, None], 'y_start_stop': [360, 700], 'xy_overlap':(0.7, 0.7)},\n",
    "     {'window_size': (96,96), 'x_start_stop': [200, 1200], 'y_start_stop': [360, 550], 'xy_overlap':(0.7, 0.7)},\n",
    "     {'window_size': (64,64), 'x_start_stop': [200, 1200], 'y_start_stop': [360, 500], 'xy_overlap':(0.7, 0.7)},\n",
    "]\n",
    "\n",
    "def search_windows(img, windows, clf, scaler, color_space=color_space, spatial_size=spatial_size, hist_bins=hist_bins, orient=orient, \n",
    "                        pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, hog_channel=hog_channel,\n",
    "                        spatial_feat=spatial_feat, hist_feat=hist_feat, hog_feat=hog_feat):\n",
    "\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        if window[1][1] < 0 or window[1][0] < 0 or window[0][1] < 0 or window[0][0] < 0:\n",
    "            continue\n",
    "\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], IMG_SHAPE)\n",
    "\n",
    "        \n",
    "        #4) Extract features for that window using single_img_features()\n",
    "        features = extract_features(test_img, color_space=color_space, spatial_size=spatial_size, hist_bins=hist_bins, orient=orient, \n",
    "                        pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, hog_channel=hog_channel,\n",
    "                        spatial_feat=spatial_feat, hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        \n",
    "        #5) Scale extracted features to be fed to classifier\n",
    "        test_features = scaler.transform(np.array(features).reshape(1, -1))\n",
    "        \n",
    "        #6) Predict using your classifier\n",
    "        prediction = clf.predict(test_features)\n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows\n",
    "\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            \n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "\n",
    "# Define a function to draw bounding boxes\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=4):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy\n",
    "\n",
    "def heatmap_from_positives(image, positive_windows):\n",
    "    heatmap = np.zeros(image.shape[0:2])\n",
    "    for window in positive_windows:\n",
    "        heatmap[window[0][1]:window[1][1], window[0][0]:window[1][0]] += 1\n",
    "    return heatmap\n",
    "\n",
    "def get_all_windows(image):\n",
    "    all_windows = []\n",
    "    for search_range in WINDOW_SEARCH_RANGES:\n",
    "        windows = slide_window(image, \n",
    "                               xy_window=search_range['window_size'],\n",
    "                               x_start_stop=search_range['x_start_stop'], \n",
    "                               y_start_stop=search_range['y_start_stop'], \n",
    "                               xy_overlap=search_range['xy_overlap'])\n",
    "        all_windows.extend(windows)\n",
    "    return all_windows\n",
    "\n",
    "# def get_var_windows(window_sizes):\n",
    "#     ret_windows = []\n",
    "#     for size in window_sizes:\n",
    "#         ret_windows.extend(slide_window(image, x_start_stop=[None, None], y_start_stop=y_start_stop, \n",
    "#                     xy_window=(size, size), xy_overlap=search_windows_overlap))\n",
    "#     return ret_windows\n",
    "\n",
    "def get_labeled_bboxes(labels):\n",
    "    bboxes = []\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        bboxes.append(bbox)\n",
    "    return bboxes\n",
    "\n",
    "def get_bbox_center(bbox):\n",
    "    x = (bbox[1][0] - bbox[0][0])/2 + bbox[0][0]\n",
    "    y = (bbox[1][1] - bbox[0][1])/2 + bbox[0][1]\n",
    "    return (int(x),int(y))\n",
    "\n",
    "# test_bb = ((100, 500), (300, 700))\n",
    "# test = get_bbox_center(test_bb)\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Try classifier on provided test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DETECTION_THRESHOLD = 4\n",
    "\n",
    "if SETTING_TEST_CLASSIFIER_ON_TEST_IMAGES:\n",
    "    files = glob('test_images/test*.jpg')\n",
    "    for file in files:\n",
    "        image = mpimg.imread(file)\n",
    "        draw_image = np.copy(image)\n",
    "\n",
    "        windows = get_all_windows(image)\n",
    "        \n",
    "        hot_windows = search_windows(image, windows, svc, X_scaler, color_space=color_space, \n",
    "                                spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                                orient=orient, pix_per_cell=pix_per_cell, \n",
    "                                cell_per_block=cell_per_block, \n",
    "                                hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                                hist_feat=hist_feat, hog_feat=hog_feat)                       \n",
    "\n",
    "\n",
    "        all_windows_img = draw_boxes(draw_image, windows)\n",
    "        positive_labeled_windows = draw_boxes(draw_image, hot_windows)\n",
    "\n",
    "        heatmap = heatmap_from_positives(image, hot_windows)\n",
    "        heatmap_orig = np.copy(heatmap)\n",
    "        # Threshold\n",
    "        heatmap[heatmap <= DETECTION_THRESHOLD] = 0\n",
    "        # Label separate detections\n",
    "        labels = label(heatmap)\n",
    "        # Calculate bounding boxes for labels\n",
    "        detected_bboxes = get_labeled_bboxes(labels)\n",
    "        # Draw bounding boxes on image\n",
    "        if detected_bboxes is not None:\n",
    "            img_with_bboxes = draw_boxes(image, detected_bboxes)\n",
    "        else: \n",
    "            img_with_bboxes = image\n",
    "\n",
    "        # Plot\n",
    "        f, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(24, 10))\n",
    "        f.tight_layout()\n",
    "        ax1.set_title('Scanned Windows', fontsize=30)\n",
    "        ax1.imshow(all_windows_img)\n",
    "        ax2.set_title('Positive Scans', fontsize=30)\n",
    "        ax2.imshow(positive_labeled_windows)\n",
    "        ax3.set_title('Heatmap before threshold', fontsize=30)\n",
    "        ax3.imshow(heatmap_orig, cmap='gray')\n",
    "        ax4.set_title('Heatmap after threshold', fontsize=30)\n",
    "        ax4.imshow(heatmap, cmap='hot')\n",
    "        ax5.set_title('Labels', fontsize=30)\n",
    "        ax5.imshow(labels[0], cmap='gray')\n",
    "        ax6.set_title('Final Detections', fontsize=30)\n",
    "        ax6.imshow(img_with_bboxes)\n",
    "        plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.1, hspace=0.2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Define and test some bounding box manipulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Use the classifier on the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full scan\n",
      "Tracker id 1 INIT rect Rect(Point(57,360),Point(207,519))\n",
      "Tracker 1 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 2 INIT rect Rect(Point(144,360),Point(207,519))\n",
      "Full scan\n",
      "Tracker 2 LOST or REJECTED\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker id 3 INIT rect Rect(Point(1112,398),Point(1156,461))\n",
      "Tracker 3 LOST or REJECTED\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker id 4 INIT rect Rect(Point(1140,416),Point(1163,511))\n",
      "Full scan\n",
      "Tracker id 5 INIT rect Rect(Point(114,360),Point(207,519))\n",
      "Tracker id 6 INIT rect Rect(Point(219,417),Point(248,480))\n",
      "Tracker 6 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 7 INIT rect Rect(Point(1026,417),Point(1051,480))\n",
      "Tracker 7 LOST or REJECTED\n",
      "Tracker 5 LOST or REJECTED\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker id 8 INIT rect Rect(Point(1040,416),Point(1051,423))\n",
      "Tracker id 9 INIT rect Rect(Point(1040,436),Point(1051,499))\n",
      "Tracker 8 LOST or REJECTED\n",
      "Tracker 9 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 10 INIT rect Rect(Point(57,408),Point(159,519))\n",
      "Tracker 10 LOST or REJECTED\n",
      "Tracker 4 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 11 INIT rect Rect(Point(1040,398),Point(1191,525))\n",
      "Tracker 11 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 12 INIT rect Rect(Point(1064,398),Point(1191,519))\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker id 13 INIT rect Rect(Point(998,436),Point(1004,499))\n",
      "Tracker 13 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 14 INIT rect Rect(Point(956,417),Point(966,442))\n",
      "Tracker 14 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 15 INIT rect Rect(Point(941,436),Point(947,442))\n",
      "Tracker id 16 INIT rect Rect(Point(950,436),Point(966,442))\n",
      "Tracker 15 LOST or REJECTED\n",
      "Tracker 16 LOST or REJECTED\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker id 17 INIT rect Rect(Point(1096,379),Point(1156,442))\n",
      "Tracker 17 LOST or REJECTED\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker 12 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 18 INIT rect Rect(Point(960,417),Point(985,480))\n",
      "Tracker id 19 INIT rect Rect(Point(1017,417),Point(1023,480))\n",
      "Tracker 19 LOST or REJECTED\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker id 20 INIT rect Rect(Point(228,408),Point(241,525))\n",
      "Tracker 20 LOST or REJECTED\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker 18 LOST or REJECTED\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker id 21 INIT rect Rect(Point(1112,408),Point(1160,423))\n",
      "Tracker id 22 INIT rect Rect(Point(1140,512),Point(1153,519))\n",
      "Tracker 21 LOST or REJECTED\n",
      "Tracker 22 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 23 INIT rect Rect(Point(950,388),Point(1229,563))\n",
      "Tracker 23 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 24 INIT rect Rect(Point(900,360),Point(1217,563))\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker 24 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 25 INIT rect Rect(Point(950,360),Point(1115,539))\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker 25 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 26 INIT rect Rect(Point(844,360),Point(967,519))\n",
      "Tracker id 27 INIT rect Rect(Point(988,416),Point(1103,511))\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker id 28 INIT rect Rect(Point(228,436),Point(255,567))\n",
      "Tracker id 29 INIT rect Rect(Point(266,436),Point(279,563))\n",
      "Tracker id 30 INIT rect Rect(Point(288,436),Point(305,563))\n",
      "Tracker id 31 INIT rect Rect(Point(96,456),Point(207,608))\n",
      "Tracker 29 LOST or REJECTED\n",
      "Tracker 28 LOST or REJECTED\n",
      "Tracker 30 LOST or REJECTED\n",
      "Tracker 31 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 32 INIT rect Rect(Point(1055,436),Point(1061,480))\n",
      "Tracker 32 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 33 INIT rect Rect(Point(1104,436),Point(1137,499))\n",
      "Tracker 33 LOST or REJECTED\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker id 34 INIT rect Rect(Point(190,360),Point(407,525))\n",
      "Tracker id 35 INIT rect Rect(Point(144,398),Point(159,519))\n",
      "Tracker id 36 INIT rect Rect(Point(38,408),Point(127,519))\n",
      "Tracker 35 LOST or REJECTED\n",
      "Tracker 34 LOST or REJECTED\n",
      "Tracker 36 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 37 INIT rect Rect(Point(114,398),Point(127,525))\n",
      "Tracker 37 LOST or REJECTED\n",
      "Tracker 27 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 38 INIT rect Rect(Point(988,398),Point(995,423))\n",
      "Tracker id 39 INIT rect Rect(Point(1026,398),Point(1135,511))\n",
      "Tracker 38 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 40 INIT rect Rect(Point(988,417),Point(995,480))\n",
      "Tracker 40 LOST or REJECTED\n",
      "Tracker 39 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 41 INIT rect Rect(Point(1008,360),Point(1156,519))\n",
      "Tracker 41 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 42 INIT rect Rect(Point(1026,388),Point(1153,525))\n",
      "Tracker id 43 INIT rect Rect(Point(304,417),Point(339,423))\n",
      "Tracker 43 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker 26 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 44 INIT rect Rect(Point(827,388),Point(925,480))\n",
      "Tracker 42 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 45 INIT rect Rect(Point(1026,398),Point(1079,483))\n",
      "Tracker id 46 INIT rect Rect(Point(1104,408),Point(1153,519))\n",
      "Tracker 45 LOST or REJECTED\n",
      "Tracker 46 LOST or REJECTED\n",
      "Full scan\n",
      "Tracker id 47 INIT rect Rect(Point(1012,388),Point(1153,525))\n",
      "Full scan\n",
      "Full scan\n",
      "Tracker 47 LOST or REJECTED\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Full scan\n",
      "Releasing everything\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def detect_cars_in_frame(image, windows):\n",
    "    hot_windows = search_windows(image, \n",
    "                                 windows, \n",
    "                                 svc, \n",
    "                                 X_scaler, \n",
    "                                 color_space=color_space, \n",
    "                                 spatial_size=spatial_size, \n",
    "                                 hist_bins=hist_bins, \n",
    "                                 orient=orient, \n",
    "                                 pix_per_cell=pix_per_cell, \n",
    "                                 cell_per_block=cell_per_block, \n",
    "                                 hog_channel=hog_channel, \n",
    "                                 spatial_feat=spatial_feat, \n",
    "                                 hist_feat=hist_feat,\n",
    "                                 hog_feat=hog_feat)                       \n",
    "\n",
    "    heatmap = heatmap_from_positives(image, hot_windows)\n",
    "    # Threshold\n",
    "    heatmap[heatmap <= DETECTION_THRESHOLD] = 0\n",
    "    # Label separate detections\n",
    "    labels = label(heatmap)\n",
    "    # Calculate bounding boxes for labels\n",
    "    detected_bboxes = get_labeled_bboxes(labels)\n",
    "    \n",
    "    return detected_bboxes\n",
    "\n",
    "from common_geometry import Rect, Point\n",
    "\n",
    "\n",
    "class VehicleTracker():\n",
    "    \n",
    "    def __init__(self, stable_id, initial_frame, initial_rect):\n",
    "        self.detect_rects_history = []\n",
    "        self.stable_id = stable_id\n",
    "        self.tracker = cv2.Tracker_create(\"KCF\")\n",
    "        tracker_bbox = (initial_rect.get_left(), \n",
    "                        initial_rect.get_top(),\n",
    "                        initial_rect.get_width(),\n",
    "                        initial_rect.get_height())\n",
    "        self.tracker.init(initial_frame, tracker_bbox)\n",
    "        self.detect_rects_history.append(initial_rect)\n",
    "        self.has_been_updated = False\n",
    "        self.is_lost = False\n",
    "        self.is_rejected = False\n",
    "\n",
    "        \n",
    "    def update(self, frame):\n",
    "        res, bbox = self.tracker.update(frame)\n",
    "        if not res:\n",
    "            self.is_lost = True\n",
    "            return None\n",
    "        rect = Rect(\n",
    "            Point(bbox[0], bbox[1]), \n",
    "            Point(bbox[0]+bbox[2], bbox[1]+bbox[3]))\n",
    "        if rect.get_height() == 0 or rect.get_width() == 0:\n",
    "            self.is_lost = True\n",
    "            return None\n",
    "        \n",
    "        # Check if classifier agrees. Weed out false positives this way.\n",
    "        classify_bbox = ((rect.p1.x, rect.p1.y), (rect.p2.x, rect.p2.y))\n",
    "        windows = []\n",
    "        windows.append(classify_bbox)\n",
    "        hot_windows = search_windows(frame, windows, svc, X_scaler, color_space=color_space, \n",
    "                    spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                    orient=orient, pix_per_cell=pix_per_cell, \n",
    "                    cell_per_block=cell_per_block, \n",
    "                    hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                    hist_feat=hist_feat, hog_feat=hog_feat)  \n",
    "        \n",
    "        if len(hot_windows) > 0:\n",
    "            # Classifier approved\n",
    "            self.detect_rects_history.append(rect)\n",
    "            self.has_been_updated = True\n",
    "            return rect\n",
    "        else:\n",
    "            # Classifier rejected. Disable this tracker\n",
    "            # TODO: Add rejection threshold?\n",
    "            self.is_rejected = True\n",
    "            return None\n",
    "    \n",
    "    def get_stable_id(self):\n",
    "        return self.stable_id\n",
    "    \n",
    "    def get_is_lost_or_rejected(self):\n",
    "        return (self.is_lost or self.is_rejected)\n",
    "    \n",
    "    def get_latest_detection(self):\n",
    "        return self.detect_rects_history[-1]\n",
    "        \n",
    "    def get_detected_rects_history(self):\n",
    "        return self.detect_rects_history\n",
    "    \n",
    "    def get_has_been_updated(self):\n",
    "        return self.has_been_updated\n",
    "\n",
    "\n",
    "FRAMES_BETWEEN_FULL_SEARCHES = 20  \n",
    "OVERLAP_THRESHOLD = 0.5\n",
    "MIN_DETECTIONS_FOR_QUALITY_TRACKER = 5\n",
    "\n",
    "def process_video(video_input_path, video_output_path):\n",
    "    if os.path.exists(video_output_path):\n",
    "        os.remove(video_output_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    video_out = cv2.VideoWriter(video_output_path, \n",
    "                                fourcc, \n",
    "                                fps=20.0, \n",
    "                                frameSize=(1280,720), \n",
    "                                isColor=True)        \n",
    "\n",
    "    video = cv2.VideoCapture(video_input_path)\n",
    "\n",
    "    has_reached_end = False\n",
    "    previous_batch_detections = None\n",
    "    full_scan_windows = None\n",
    "    frame_idx = 0\n",
    "    trackers = []\n",
    "    historical_lost_trackers = []\n",
    "    vehicles_counter = 0\n",
    "    too_many_lost_trackers = False\n",
    "    while(video.isOpened() and not has_reached_end):\n",
    "        time_op_start = time.time()\n",
    "\n",
    "        # Read a frame, make sure we're not at the end\n",
    "        ret, frame = video.read()\n",
    "        if ret == False:\n",
    "            has_reached_end = True\n",
    "            break\n",
    "\n",
    "        # Get the full scan windows for the first time if needed\n",
    "        if full_scan_windows is None:\n",
    "            full_scan_windows = get_all_windows(frame)\n",
    "\n",
    "        # For FRAMES_BETWEEN_FULL_SEARCHES frames, do full detection. \n",
    "        # Otherwise, just continue detecting using the trackers\n",
    "        frame_detections = []\n",
    "        if (frame_idx % FRAMES_BETWEEN_FULL_SEARCHES == 0) or too_many_lost_trackers:\n",
    "            print(\"Full scan\")\n",
    "            too_many_lost_trackers = False\n",
    "            frame_detections = detect_cars_in_frame(\n",
    "                frame,\n",
    "                full_scan_windows\n",
    "            )\n",
    "\n",
    "            for detection in frame_detections:\n",
    "                rect = Rect(\n",
    "                    Point(detection[0][0], detection[0][1]),\n",
    "                    Point(detection[1][0], detection[1][1]))\n",
    "                if rect.get_height() == 0 or rect.get_width() == 0:\n",
    "                    continue\n",
    "                current_detection_area = rect.calculate_area()\n",
    "\n",
    "                # Check if there is overlap with an existing tracker\n",
    "                overlapping_tracker_found = False\n",
    "                for tracker in trackers:\n",
    "                    if tracker.get_is_lost_or_rejected() == True:\n",
    "                        continue\n",
    "                    latest_detection = tracker.get_latest_detection()\n",
    "                    overlap = latest_detection.calculate_overlap(rect)\n",
    "                    if overlap == None:\n",
    "                        continue\n",
    "                    overlap_area = overlap.calculate_area()\n",
    "                    if overlap_area/current_detection_area >= OVERLAP_THRESHOLD:\n",
    "                        # Found an overlapping tracker, no need to record this detection\n",
    "                        overlapping_tracker_found = True\n",
    "                        break\n",
    "                    elif overlap_area/latest_detection.calculate_area() >= OVERLAP_THRESHOLD:\n",
    "                        # Overlap is large in comparison to previous detected area\n",
    "                        # TODO: Consider switching to the larger area\n",
    "                        overlapping_tracker_found = True\n",
    "                        break   \n",
    "                # No need to create a new tracker if the full scan window overlaps with an existing tracker\n",
    "                if overlapping_tracker_found == True:\n",
    "                    continue\n",
    "\n",
    "                vehicles_counter += 1\n",
    "                tracker = VehicleTracker(vehicles_counter, frame, rect)\n",
    "                print(\"Tracker id {} INIT rect {}\".format(tracker.get_stable_id(), rect))\n",
    "                trackers.append(tracker)\n",
    "\n",
    "        # We count on the trackers to provide us with detections\n",
    "        # We validate detections against our own classifier\n",
    "        # If the classifier rejects the detection, \n",
    "        frame_detections = []\n",
    "        for tracker in trackers:\n",
    "    #         # Skip updating trackers which were just initialized and never have been updated yet\n",
    "    #         rect = None\n",
    "    #         if tracker.get_has_been_updated():\n",
    "    #             rect = tracker.update(frame)\n",
    "    #         else:\n",
    "            rect = tracker.update(frame)\n",
    "            if rect is not None:\n",
    "    #             print(\"Tracker {} FIND bbox: {}\".format(tracker.get_stable_id(), rect))\n",
    "                bbox = ((rect.p1.x, rect.p1.y), (rect.p2.x, rect.p2.y))\n",
    "                frame_detections.append(bbox)\n",
    "            else:\n",
    "                print(\"Tracker {} LOST or REJECTED\".format(tracker.get_stable_id()))\n",
    "\n",
    "        # Move lost trackers to historical_lost_trackers\n",
    "        lost_trackers = [tracker for tracker in trackers if tracker.get_is_lost_or_rejected()]\n",
    "        historical_lost_trackers.extend(lost_trackers)\n",
    "        trackers = [tracker for tracker in trackers if not tracker.get_is_lost_or_rejected()]\n",
    "\n",
    "        # TODO: Go over each tracker, if there are more than 2 recent detections, draw the ID on the car\n",
    "        \n",
    "        # TODO: For lost trackers which had enough detections, we want to run nearby classifier search, to attempt a reacquire\n",
    "        # Until then, we just run a full search if a high quality tracker was just lost\n",
    "        quality_lost_trackers = [tracker for tracker in lost_trackers if len(tracker.get_detected_rects_history()) >= MIN_DETECTIONS_FOR_QUALITY_TRACKER ]\n",
    "        if len(quality_lost_trackers) > 0:\n",
    "            too_many_lost_trackers = True\n",
    "        \n",
    "        frame_idx += 1\n",
    "        out_frame = draw_boxes(frame, frame_detections)\n",
    "        video_out.write(out_frame)\n",
    "\n",
    "        # TODO: \n",
    "        # 1. (2d) velocity and (2d) direction calculation. Can later be used for 3d calculation by transform to horizon\n",
    "\n",
    "\n",
    "\n",
    "        # Original pipeline time per frame was 9 seconds on my laptop.\n",
    "        # To build a real-time pipeline we need 2 orders of magnitute improvement (<16ms for 60fps)\n",
    "        time_op_end = time.time()\n",
    "    #     print('Time per frame: {}', (time_op_end - time_op_start))\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Releasing everything\")\n",
    "    video.release()\n",
    "    video_out.release()\n",
    "    # cv2.destroyAllWindows() ro\n",
    "\n",
    "# Process test video\n",
    "TEST_VIDEO_INPUT_PATH = 'test_video.mp4'\n",
    "TEST_VIDEO_OUTPUT_PATH = 'output_images/test_video_output.mov'\n",
    "# process_video(TEST_VIDEO_INPUT_PATH, TEST_VIDEO_OUTPUT_PATH)\n",
    "\n",
    "PROJECT_VIDEO_INPUT_PATH = 'project_video.mp4'\n",
    "PROJECT_VIDEO_OUTPUT_PATH = 'output_images/project_video_output.mov'\n",
    "process_video(PROJECT_VIDEO_INPUT_PATH, PROJECT_VIDEO_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # import IPython.display as disp\n",
    "# import base64\n",
    "# # disp.Video()\n",
    "# def video(fname, mimetype):\n",
    "#     from IPython.display import HTML\n",
    "#     video_not_encoded = open(fname, \"rb\").read()\n",
    "#     video_encoded = base64.b64encode(video_not_encoded)\n",
    "#     video_tag = '<video controls alt=\"test\" src=\"data:video/{0};base64,{1}\">'.format(mimetype, video_encoded.decode())\n",
    "#     return HTML(data=video_tag)\n",
    "\n",
    "# video(\"test_video_processed.avi\", \"avi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Trackers to try: KCF, MEDIANFLOW, GOTURN?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# NEARBY_WINDOW_OVERLAP_HORIZ = 0.95\n",
    "# NEARBY_WINDOW_OVERLAP_VERT = 0.98\n",
    "# NEARBY_WINDOW_STEPS_PER_DIRECTION = 2\n",
    "# NEARBY_WINDOW_SCALEDOWN = 0.98\n",
    "\n",
    "\n",
    "# Inside the frame loop:\n",
    "\n",
    "#     # Get a batch of frames of size FRAMES_BATCH_SIZE\n",
    "#     frames_batch = []\n",
    "#     for i in range(0, FRAMES_BATCH_SIZE):    \n",
    "#         ret, frame = video.read()\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             has_reached_end = True\n",
    "#             b reak\n",
    "#         if ret==True:\n",
    "#             frames_batch.append(frame)\n",
    "#         else:\n",
    "#             has_reached_end = True\n",
    "#             break\n",
    "        \n",
    "#     if len(frames_batch) == 0:\n",
    "#         break\n",
    "        \n",
    "#     if full_scan_windows is None:\n",
    "#         full_scan_windows = get_all_windows(frames_batch[0])\n",
    "        \n",
    "#     detections_per_frame = {}\n",
    "#     first_frame_detections = detect_cars_in_frame(\n",
    "#         frames_batch[0],\n",
    "#         full_scan_windows\n",
    "#     )\n",
    "#     detections_per_frame[0] = first_frame_detections\n",
    "#     for i in range(1,len(frames_batch)):\n",
    "#         prev_frame_detections = detections_per_frame[i-1]\n",
    "#         frame_search_windows = []\n",
    "#         for detection in prev_frame_detections:\n",
    "#             nearby_windows = bbox_to_nearby(detection, \n",
    "#                                             NEARBY_WINDOW_STEPS_PER_DIRECTION, \n",
    "#                                             NEARBY_WINDOW_OVERLAP_HORIZ,\n",
    "#                                             NEARBY_WINDOW_OVERLAP_VERT,\n",
    "#                                             NEARBY_WINDOW_SCALEDOWN, \n",
    "#                                             ((0,0), (1280, 720))\n",
    "#                                            )\n",
    "#             frame_search_windows.extend(nearby_windows)\n",
    "            \n",
    "#         frame_detections = detect_cars_in_frame(\n",
    "#             frames_batch[i], \n",
    "#             frame_search_windows\n",
    "#         )   \n",
    "#         detections_per_frame[i] = frame_detections\n",
    "#         print(frame_detections)\n",
    "#         frames_batch[i] = draw_boxes(frames_batch[i], frame_detections)\n",
    "    \n",
    "#     # Write resulting frames\n",
    "#     for out_frame in frames_batch:\n",
    "#         video_out.write(out_frame)\n",
    "#     print('Time per frame: {}', (time_op_end - time_op_start)/len(frames_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car blue \n"
     ]
    }
   ],
   "source": [
    "for i in range(1,2):\n",
    "    print (\"car blue \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time 12:45 pm car black do not "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# NEARBY_WINDOW_OVERLAP_HORIZ = 0.95\n",
    "# NEARBY_WINDOW_OVERLAP_VERT = 0.98\n",
    "# NEARBY_WINDOW_STEPS_PER_DIRECTION = 2\n",
    "# NEARBY_WINDOW_SCALEDOWN = 0.98\n",
    "\n",
    "\n",
    "# Inside the frame loop:\n",
    "\n",
    "#     # Get a batch of frames of size FRAMES_BATCH_SIZE\n",
    "#     frames_batch = []\n",
    "#     for i in range(0, FRAMES_BATCH_SIZE):    \n",
    "#         ret, frame = video.read()\n",
    "#         if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#             has_reached_end = True\n",
    "#             break\n",
    "#         if ret==True:\n",
    "#             frames_batch.append(frame)\n",
    "#         else:\n",
    "#             has_reached_end = True\n",
    "#             break\n",
    "        \n",
    "#     if len(frames_batch) == 0:\n",
    "#         break\n",
    "        \n",
    "#     if full_scan_windows is None:\n",
    "#         full_scan_windows = get_all_windows(frames_batch[0])\n",
    "        \n",
    "#     detections_per_frame = {}\n",
    "#     first_frame_detections = detect_cars_in_frame(\n",
    "#         frames_batch[0],\n",
    "#         full_scan_windows\n",
    "#     )\n",
    "#     detections_per_frame[0] = first_frame_detections\n",
    "#     for i in range(1,len(frames_batch)):\n",
    "#         prev_frame_detections = detections_per_frame[i-1]\n",
    "#         frame_search_windows = []\n",
    "#         for detection in prev_frame_detections:\n",
    "#             nearby_windows = bbox_to_nearby(detection, \n",
    "#                                             NEARBY_WINDOW_STEPS_PER_DIRECTION, \n",
    "#                                             NEARBY_WINDOW_OVERLAP_HORIZ,\n",
    "#                                             NEARBY_WINDOW_OVERLAP_VERT,\n",
    "#                                             NEARBY_WINDOW_SCALEDOWN, \n",
    "#                                             ((0,0), (1280, 720))\n",
    "#                                            )\n",
    "#             frame_search_windows.extend(nearby_windows)\n",
    "            \n",
    "#         frame_detections = detect_cars_in_frame(\n",
    "#             frames_batch[i], \n",
    "#             frame_search_windows\n",
    "#         )   \n",
    "#         detections_per_frame[i] = frame_detections\n",
    "#         print(frame_detections)\n",
    "#         frames_batch[i] = draw_boxes(frames_batch[i], frame_detections)\n",
    "    \n",
    "#     # Write resulting frames\n",
    "#     for out_frame in frames_batch:\n",
    "#         video_out.write(out_frame)\n",
    "#     print('Time per frame: {}', (time_op_end - time_op_start)/len(frames_batch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
